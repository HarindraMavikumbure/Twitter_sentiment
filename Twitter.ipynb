{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (4.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from transformers) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: emoji==0.6.0 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\pycharmprojects\\sentiment_colab\\venv\\lib\\site-packages (from click->nltk) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Twitter_sentiment' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji\n",
    "!pip install transformers\n",
    "!pip install nltk emoji==0.6.0\n",
    "!git clone https://github.com/HarindraMavikumbure/Twitter_sentiment\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tClassification Report for Naive Bayes:\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Strong Negative       0.00      0.00      0.00         6\n",
      "       Negative       0.32      0.56      0.41       112\n",
      "        Neutral       0.62      0.26      0.36       256\n",
      "       Positive       0.36      0.48      0.41       101\n",
      "Strong Positive       0.04      0.33      0.07         3\n",
      "\n",
      "      micro avg       0.37      0.37      0.37       478\n",
      "      macro avg       0.27      0.33      0.25       478\n",
      "   weighted avg       0.49      0.37      0.38       478\n",
      "    samples avg       0.37      0.37      0.37       478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import configparser\n",
    "\n",
    "#data processing\n",
    "import re, string\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Naive Bayes\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#transformers\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import TFBertModel\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import TFRobertaModel\n",
    "\n",
    "#keras\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from emoji import demojize\n",
    "\n",
    "#set seed for reproducibility\n",
    "seed=42\n",
    "MAX_LEN=128\n",
    "\n",
    "\n",
    "def create_model(bert_model, max_len=MAX_LEN):\n",
    "\n",
    "    ##params###\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-7)\n",
    "    loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "    accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
    "\n",
    "\n",
    "    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
    "\n",
    "    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n",
    "\n",
    "    embeddings = bert_model([input_ids,attention_masks])[1]\n",
    "\n",
    "    output = tf.keras.layers.Dense(5, activation=\"softmax\")(embeddings)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks], outputs = output)\n",
    "\n",
    "    model.compile(opt, loss=loss, metrics=accuracy)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def tokenize(data,max_len=MAX_LEN) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for i in range(len(data)):\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            data[i],\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids),np.array(attention_masks)\n",
    "\n",
    "def tokenize_roberta(data,max_len=MAX_LEN) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for i in range(len(data)):\n",
    "        encoded = tokenizer_roberta.encode_plus(\n",
    "            data[i],\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids),np.array(attention_masks)\n",
    "\n",
    "##CUSTOM DEFINED FUNCTIONS TO CLEAN THE TWEETS\n",
    "\n",
    "#Clean emojis from text\n",
    "def strip_emoji(text):\n",
    "    return demojize(text) #remove emoji\n",
    "\n",
    "#Remove punctuations, links, mentions and \\r\\n new line characters\n",
    "def strip_all_entities(text):\n",
    "    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
    "    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    return text\n",
    "\n",
    "#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\n",
    "def clean_hashtags(tweet):\n",
    "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n",
    "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n",
    "    return new_tweet2\n",
    "\n",
    "#Filter special characters such as & and $ present in some words\n",
    "def filter_chars(a):\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    return ' '.join(sent)\n",
    "\n",
    "def remove_mult_spaces(text): # remove multiple spaces\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)\n",
    "\n",
    "#INI Stuff\n",
    "\n",
    "# config section, variables from config.ini\n",
    "config = configparser.ConfigParser()\n",
    "config.read('Twitter_sentiment/src/config.ini')\n",
    "\n",
    "# INI file variables, descriptions in INI file\n",
    "model_type = int(config[\"MODEL\"]['MODEL_TYPE'])\n",
    "batch_size = int(config[\"MODEL\"]['BATCH_SIZE'])\n",
    "epochs = int(config[\"MODEL\"]['EPOCHS'])\n",
    "category = int(config[\"CATEGORY\"]['TARGET'])\n",
    "test_type = int(config[\"CATEGORY\"]['TEST'])\n",
    "\n",
    "\n",
    "csv_path = config[\"CSV\"]['PATH']\n",
    "train_list = [\"individuals_train_dev.csv\", \"groups_train_dev.csv\", \"events_train_dev.csv\"]\n",
    "test_list = [\"individuals_test.csv\", \"groups_test.csv\", \"events_test.csv\", \"all_test.csv\"]\n",
    "\n",
    "train_string = csv_path + train_list[category]\n",
    "test_string = csv_path + test_list[test_type]\n",
    "\n",
    "# Prepare Dataframe\n",
    "\n",
    "df_train = pd.read_csv(train_string,encoding='utf-8')\n",
    "df_test = pd.read_csv(test_string,encoding='utf-8')\n",
    "\n",
    "df_train.head()\n",
    "df_test.head()\n",
    "df = df_train[['clean_text','category']]\n",
    "df_test = df_test[['clean_text','category']]\n",
    "\n",
    "texts_new = []\n",
    "for t in df.clean_text:\n",
    "    texts_new.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))\n",
    "\n",
    "texts_new_test = []\n",
    "for t in df_test.clean_text:\n",
    "    texts_new_test.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))\n",
    "\n",
    "df['text_clean'] = texts_new\n",
    "df_test['text_clean'] = texts_new_test\n",
    "\n",
    "text_len = []\n",
    "for text in df.text_clean:\n",
    "    tweet_len = len(text.split())\n",
    "    text_len.append(tweet_len)\n",
    "\n",
    "df['text_len'] = text_len\n",
    "\n",
    "text_len_test = []\n",
    "for text in df_test.text_clean:\n",
    "    tweet_len = len(text.split())\n",
    "    text_len_test.append(tweet_len)\n",
    "\n",
    "df_test['text_len'] = text_len_test\n",
    "\n",
    "df['text_clean'].head()\n",
    "df_test['text_clean'].head()\n",
    "\n",
    "#Tokenize\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "token_lens_test = []\n",
    "\n",
    "for i,txt in enumerate(df_test['text_clean'].values):\n",
    "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
    "    token_lens_test.append(len(tokens))\n",
    "    if len(tokens)>80:\n",
    "        print(f\"INDEX: {i}, TEXT: {txt}\")\n",
    "\n",
    "# Ready Dataframes for every type of model\n",
    "\n",
    "df_test['token_lens'] = token_lens_test\n",
    "df_test = df_test.sort_values(by='token_lens', ascending=False)\n",
    "df_test.head(10)\n",
    "df_test = df_test.iloc[5:]\n",
    "df_test.head(3)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['category'].value_counts()\n",
    "\n",
    "ros = RandomOverSampler()\n",
    "train_x, train_y = ros.fit_resample(np.array(df['text_clean']).reshape(-1, 1), np.array(df['category']).reshape(-1, 1));\n",
    "train_os = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['text_clean', 'category']);\n",
    "\n",
    "\n",
    "X = train_os['text_clean'].values\n",
    "y = train_os['category'].values\n",
    "\n",
    "# Use seed to ensure the train/dev split is the same each run\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, stratify=y, random_state=seed)\n",
    "\n",
    "X_test = df_test['text_clean'].values\n",
    "y_test = df_test['category'].values\n",
    "\n",
    "# Used for Naive Bayes\n",
    "y_train_le = y_train.copy()\n",
    "y_valid_le = y_valid.copy()\n",
    "y_test_le = y_test.copy()\n",
    "\n",
    "X_test = df_test['text_clean'].values\n",
    "y_test = df_test['category'].values\n",
    "\n",
    "#Avoid fit transform for smaller sets, as they may be reshaped if extreme values are missing\n",
    "ohe = preprocessing.OneHotEncoder()\n",
    "y_train = ohe.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n",
    "y_valid = ohe.transform(np.array(y_valid).reshape(-1, 1)).toarray()\n",
    "y_test = ohe.transform(np.array(y_test).reshape(-1, 1)).toarray()\n",
    "\n",
    "#Naive Bayes\n",
    "if model_type == 0:\n",
    "\n",
    "    clf = CountVectorizer()\n",
    "    X_train_cv =  clf.fit_transform(X_train)\n",
    "    X_test_cv = clf.transform(X_test)\n",
    "\n",
    "\n",
    "    tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_cv)\n",
    "    X_train_tf = tf_transformer.transform(X_train_cv)\n",
    "    X_test_tf = tf_transformer.transform(X_test_cv)\n",
    "\n",
    "\n",
    "    nb_clf = MultinomialNB()\n",
    "\n",
    "    nb_clf.fit(X_train_tf, y_train_le)\n",
    "\n",
    "\n",
    "    nb_pred = nb_clf.predict(X_test_tf)\n",
    "\n",
    "\n",
    "    #this is a hack to get the classification report to display al classes and not just predicted classes\n",
    "    nb_pred_all_classes = ohe.transform(np.array(nb_pred).reshape(-1, 1)).toarray()\n",
    "\n",
    "    print('\\tClassification Report for Naive Bayes:\\n\\n',classification_report(y_test,nb_pred_all_classes, target_names=['Strong Negative', 'Negative', 'Neutral', 'Positive', 'Strong Positive']))\n",
    "\n",
    "# BERT MODEL\n",
    "\n",
    "if model_type == 1:\n",
    "\n",
    "    train_input_ids, train_attention_masks = tokenize(X_train, MAX_LEN)\n",
    "    val_input_ids, val_attention_masks = tokenize(X_valid, MAX_LEN)\n",
    "    test_input_ids, test_attention_masks = tokenize(X_test, MAX_LEN)\n",
    "\n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "    model = create_model(bert_model, MAX_LEN)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    history_bert = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_valid), epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    result_bert = model.predict([test_input_ids,test_attention_masks])\n",
    "\n",
    "\n",
    "    y_pred_bert =  np.zeros_like(result_bert)\n",
    "    y_pred_bert[np.arange(len(y_pred_bert)), result_bert.argmax(1)] = 1\n",
    "\n",
    "    print('\\tClassification Report for BERT:\\n\\n',classification_report(y_test,y_pred_bert, target_names=['Strong Negative', 'Negative', 'Neutral', 'Positive', 'Strong Positive']))\n",
    "\n",
    "\n",
    "# ROBERTA MODEL\n",
    "\n",
    "if model_type == 2:\n",
    "    tokenizer_roberta = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "    token_lens = []\n",
    "\n",
    "    for txt in X_train:\n",
    "        tokens = tokenizer_roberta.encode(txt, max_length=512, truncation=True)\n",
    "        token_lens.append(len(tokens))\n",
    "    max_length=np.max(token_lens)\n",
    "    max_length\n",
    "\n",
    "\n",
    "    train_input_ids, train_attention_masks = tokenize_roberta(X_train, MAX_LEN)\n",
    "    val_input_ids, val_attention_masks = tokenize_roberta(X_valid, MAX_LEN)\n",
    "    test_input_ids, test_attention_masks = tokenize_roberta(X_test, MAX_LEN)\n",
    "\n",
    "\n",
    "    roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "    model = create_model(roberta_model, MAX_LEN)\n",
    "    model.summary()\n",
    "\n",
    "    history_2 = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_valid), epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    result_roberta = model.predict([test_input_ids,test_attention_masks])\n",
    "\n",
    "    y_pred_roberta =  np.zeros_like(result_roberta)\n",
    "    y_pred_roberta[np.arange(len(y_pred_roberta)), result_roberta.argmax(1)] = 1\n",
    "\n",
    "\n",
    "    print('\\tClassification Report for RoBERTa:\\n\\n',classification_report(y_test,y_pred_roberta, target_names=['Strong Negative', 'Negative', 'Neutral', 'Positive', \"Strong Positive\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}